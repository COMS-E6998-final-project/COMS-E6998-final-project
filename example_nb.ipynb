{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "gJltvRy052pC",
      "metadata": {
        "id": "gJltvRy052pC"
      },
      "source": [
        "## Note on Reproducibility and Scope\n",
        "\n",
        "This notebook serves as an illustration of the model-compression techniques explored in our project. Due to the use of proprietary Google infrastructure in the original experiments, the full production pipeline cannot be released publicly. Here we provide a small demo of the knowledge distillation + quantization flow.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "P8oCFIMm53Yp",
      "metadata": {
        "id": "P8oCFIMm53Yp"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "e18893f9",
      "metadata": {
        "id": "e18893f9"
      },
      "source": [
        "### Setup\n",
        "\n",
        "In Google Ads' internal conversion value prediction system, a large teacher model predicts conversion outcomes using a multi-head architecture and modeling performance using a Poisson Log Loss function.\n",
        "\n",
        "The student model is a smaller, more efficient neural network designed to mimic the teacher's behavior. Through knowledge distillation, the student learns to approximate the teacher's predictions while using significantly fewer parameters, enabling faster inference and lower serving costs in production.\n",
        "\n",
        "To illustrate this pipeline, we define a synthetic conversion prediction task. Each training sample consists of:\n",
        "  - A random feature vector\n",
        "  - A binary label indicating whether a conversion occurred\n",
        "  - A non-negative conversion count (with many zeros, reflecting real-world sparsity)\n",
        "\n",
        "---\n",
        "\n",
        "### Model Architectures\n",
        "\n",
        "Teacher Model\n",
        "- Architecture: [128, 64, 32] - three dense hidden layers\n",
        "- Three output heads:\n",
        "  - `conv_prob`: Sigmoid activation (conversion probability)\n",
        "  - `conv_value`: Softplus activation (non-negative count for Poisson loss)\n",
        "  - `prob_logits`: Raw logits for distillation\n",
        "\n",
        "Student Model\n",
        "- Architecture: [64, 32] - two hidden layers (fixed to ensure dimension compatibility with the teacher's bottleneck layer)\n",
        "- Same output heads and activations as the teacher\n",
        "\n",
        "---\n",
        "### Hyperparameter Sweep Configuration\n",
        "\n",
        "We include an example config with sweeps to show how the experiments can pull from the config and sweep over the specified hyperparamters. This example is very simple compared to the more sophisticated pyplan config options at Google, but demonstrates the high-level workflow.\n",
        "\n",
        "This setup demonstrates how model compression via teacher-student training can retain accuracy while significantly improving deployment efficiency in large-scale systems such as Google Ads.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "fcce0fe6",
      "metadata": {
        "id": "fcce0fe6"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import itertools\n",
        "import os\n",
        "\n",
        "# example of user config input (in reality would be separate file)\n",
        "HYPERPARAM_CONFIG = {\n",
        "    \"student\": {\n",
        "        \"dropout_rate\": [0.0, 0.1, 0.2]\n",
        "    },\n",
        "    \"distillation\": {\n",
        "        \"alpha\": [0.1, 0.5],\n",
        "        \"temperature\": [3.0]\n",
        "    }\n",
        "}\n",
        "\n",
        "# fix random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "num_features = 10\n",
        "num_samples = 10000\n",
        "X_train = np.random.normal(size=(num_samples, num_features)).astype(np.float32)\n",
        "\n",
        "# simulate a scenario where conversion count follows a Poisson distribution based on features.\n",
        "# define an arbitrary \"true\" weight vector for Poisson rate:\n",
        "true_w = np.random.normal(scale=0.5, size=(num_features, 1))\n",
        "\n",
        "# compute a latent log-rate and apply exp to get Poisson lambda\n",
        "log_lambda = X_train.dot(true_w)  # shape (num_samples, 1)\n",
        "lambda_vals = np.exp(log_lambda).flatten()\n",
        "\n",
        "# sample conversion counts from Poisson(lambda). (Clip to some max for safety)\n",
        "y_count = np.random.poisson(lam=lambda_vals).astype(np.float32)\n",
        "y_count = np.clip(y_count, 0, 20)  # limit extreme values for stability\n",
        "\n",
        "# conversion occurred or not (binary label) – 1 if count > 0 else 0\n",
        "y_conv = (y_count > 0).astype(np.float32)\n",
        "\n",
        "# prepare targets for Keras\n",
        "y_train = {\n",
        "    \"conv_prob\": y_conv,\n",
        "    \"conv_value\": y_count,\n",
        "    \"prob_logits\": np.zeros_like(y_conv)\n",
        "}\n",
        "\n",
        "def build_model(hidden_units, dropout=0.0, name=\"Model\"):\n",
        "    inputs = tf.keras.Input(shape=(10,), name=\"features\")\n",
        "    x = inputs\n",
        "\n",
        "    for i, units in enumerate(hidden_units):\n",
        "        layer_name = \"bottleneck\" if i == len(hidden_units) - 1 else f\"hidden_{i}\"\n",
        "        x = tf.keras.layers.Dense(units, activation='relu', name=layer_name)(x)\n",
        "        if dropout > 0:\n",
        "            x = tf.keras.layers.Dropout(dropout)(x)\n",
        "\n",
        "    prob_logits = tf.keras.layers.Dense(1, activation=None, name=\"prob_logits\")(x)\n",
        "    prob_output = tf.keras.layers.Activation('sigmoid', name=\"conv_prob\")(prob_logits)\n",
        "    value_output = tf.keras.layers.Dense(1, activation=tf.nn.softplus, name=\"conv_value\")(x)\n",
        "\n",
        "    # return a cictionary of outputs to match the keys in loss/y_train\n",
        "    return tf.keras.Model(\n",
        "        inputs=inputs,\n",
        "        outputs={\n",
        "            \"conv_prob\": prob_output,\n",
        "            \"conv_value\": value_output,\n",
        "            \"prob_logits\": prob_logits\n",
        "        },\n",
        "        name=name\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "49b46675",
      "metadata": {
        "id": "49b46675"
      },
      "source": [
        "### Synthetic Training Setup\n",
        "\n",
        "We generate random input features and construct labels to mimic a realistic conversion prediction task:\n",
        "\n",
        "- The conversion count `y_count` is sampled from a Poisson distribution:  \n",
        "  $$\n",
        "  y_{\\text{count}} \\sim \\text{Poisson}(\\exp(w \\cdot x))\n",
        "  $$\n",
        "- The binary conversion label `y_conv` is defined as:\n",
        "  $$\n",
        "  y_{\\text{conv}} = \\mathbb{1}(y_{\\text{count}} > 0)\n",
        "  $$\n",
        "\n",
        "---\n",
        "\n",
        "### Teacher Model Training Objective\n",
        "\n",
        "The teacher model is trained using two loss functions:\n",
        "\n",
        "- Binary Cross-Entropy Loss for the `conv_prob` output, modeling the probability of a conversion\n",
        "- Poisson Loss (`tf.keras.losses.Poisson`) for the `conv_value` output, modeling the conversion count or value\n",
        "\n",
        "The Poisson loss in TensorFlow implements the negative Poisson log-likelihood:\n",
        "$$\n",
        "L = y_{\\text{pred}} - y_{\\text{true}} \\log(y_{\\text{pred}})\n",
        "$$\n",
        "which matches the standard Poisson log loss up to constant terms that do not affect optimization.\n",
        "\n",
        "Here, the teacher model is trained briefly on synthetic data. In a real production setting, the teacher would be pre-trained on a massive dataset with richer features and longer training schedules.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "a24af986",
      "metadata": {
        "id": "a24af986"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "5fdde33f",
      "metadata": {
        "id": "5fdde33f"
      },
      "source": [
        "## 2. Knowledge Distillation Training (Hard + Soft Targets)\n",
        "\n",
        "Knowledge distillation (KD) compresses a large teacher model into a smaller student by training the student on both:\n",
        "- Hard targets (ground-truth labels), and  \n",
        "- Soft targets (the teacher's predictions).\n",
        "\n",
        "The key idea is that the teacher's outputs encode *dark knowledge* about the underlying function, helping the student generalize better than training on labels alone.\n",
        "\n",
        "---\n",
        "\n",
        "### Distillation Loss\n",
        "\n",
        "The student is trained with a weighted combination of losses:\n",
        "$$\n",
        "L_{\\text{total}} = \\alpha \\, L_{\\text{hard}}(y_{\\text{true}}, y_{\\text{student}})\n",
        "+ \\beta \\, L_{\\text{soft}}(y_{\\text{teacher}}, y_{\\text{student}})\n",
        "$$\n",
        "where alpha and beta balance supervision from true labels and the teacher (often alpha + beta = 1).\n",
        "\n",
        "- Hard losses:\n",
        "  - Binary cross-entropy for conversion probability  \n",
        "  - Poisson loss for conversion count  \n",
        "- Soft losses:\n",
        "  - Mean squared error (MSE) between teacher and student outputs for each head  \n",
        "\n",
        "The teacher's weights are frozen, and the student is trained using the combined distillation loss.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c6d8cf84",
      "metadata": {
        "id": "c6d8cf84",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "def run_experiment(dropout, alpha, temp, X_train, y_train, teacher_weights):\n",
        "\n",
        "    # build student and teacher\n",
        "    student_model = build_model([64, 32], dropout=dropout, name=\"Student\")\n",
        "    teacher_model = build_model([128, 64, 32], name=\"Teacher\")\n",
        "    teacher_model.set_weights(teacher_weights)\n",
        "    teacher_model.trainable = False\n",
        "\n",
        "    # feature extractors\n",
        "    teacher_feat_extractor = tf.keras.Model(inputs=teacher_model.inputs, outputs=teacher_model.get_layer(\"bottleneck\").output)\n",
        "    student_feat_extractor = tf.keras.Model(inputs=student_model.inputs, outputs=student_model.get_layer(\"bottleneck\").output)\n",
        "\n",
        "    # training loop\n",
        "    batch_size = 64\n",
        "    epochs = 2\n",
        "    learning_rate = 0.001\n",
        "    gamma = 0.1\n",
        "\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(batch_size)\n",
        "\n",
        "    final_loss = 0.0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        epoch_loss = 0.0\n",
        "        steps = 0\n",
        "        for x_batch, y_batch in dataset:\n",
        "            # teacher inference\n",
        "            t_preds = teacher_model(x_batch, training=False)\n",
        "            t_prob, t_val, t_logits = t_preds[\"conv_prob\"], t_preds[\"conv_value\"], t_preds[\"prob_logits\"]\n",
        "\n",
        "            with tf.GradientTape() as tape:\n",
        "                # student training (dictionary output)\n",
        "                s_preds = student_model(x_batch, training=True)\n",
        "                s_prob = s_preds[\"conv_prob\"]\n",
        "                s_val = s_preds[\"conv_value\"]\n",
        "                s_logits = s_preds[\"prob_logits\"]\n",
        "\n",
        "                # squeeze outputs to match label dimensions\n",
        "                s_prob_squeezed = tf.squeeze(s_prob, axis=-1)\n",
        "                s_val_squeezed = tf.squeeze(s_val, axis=-1)\n",
        "\n",
        "                # hard loss (compare student output to ground truth labels)\n",
        "                loss_hard = tf.reduce_mean(tf.keras.losses.binary_crossentropy(y_batch[\"conv_prob\"], s_prob_squeezed)) + \\\n",
        "                            tf.reduce_mean(tf.keras.losses.Poisson()(y_batch[\"conv_value\"], s_val_squeezed))\n",
        "\n",
        "                # soft loss (compare student logits to teacher logits)\n",
        "                t_prob_soft = tf.nn.sigmoid(t_logits / temp)\n",
        "                s_prob_soft = tf.nn.sigmoid(s_logits / temp)\n",
        "                mse = tf.keras.losses.MeanSquaredError()\n",
        "                loss_soft = tf.reduce_mean(mse(t_prob_soft, s_prob_soft)) + \\\n",
        "                            tf.reduce_mean(mse(t_val, s_val))\n",
        "\n",
        "                # feature loss\n",
        "                feat_loss = 0.0\n",
        "                if gamma > 0:\n",
        "                    t_feat = teacher_feat_extractor(x_batch, training=False)\n",
        "                    s_feat = student_feat_extractor(x_batch, training=True)\n",
        "                    feat_loss = tf.reduce_mean(mse(t_feat, s_feat))\n",
        "\n",
        "                total_loss = (1 - alpha) * loss_hard + (alpha * (temp**2) * loss_soft) + (gamma * feat_loss)\n",
        "\n",
        "            grads = tape.gradient(total_loss, student_model.trainable_weights)\n",
        "            optimizer.apply_gradients(zip(grads, student_model.trainable_weights))\n",
        "            epoch_loss += total_loss\n",
        "            steps += 1\n",
        "\n",
        "        final_loss = float(epoch_loss / steps)\n",
        "\n",
        "    return final_loss, student_model\n",
        "\n",
        "\n",
        "def generate_configs():\n",
        "    dropout_list = HYPERPARAM_CONFIG['student']['dropout_rate']\n",
        "    alpha_list = HYPERPARAM_CONFIG['distillation']['alpha']\n",
        "    temp_list = HYPERPARAM_CONFIG['distillation']['temperature']\n",
        "\n",
        "    for dropout, alpha, temp in itertools.product(dropout_list, alpha_list, temp_list):\n",
        "        label = f\"Dropout={dropout}, Alpha={alpha}, Temp={temp}\"\n",
        "        yield (dropout, alpha, temp), label\n",
        "\n",
        "\n",
        "## train teacher (stays constant)\n",
        "print(\"Training Base Teacher ([128, 64, 32])...\")\n",
        "base_teacher = build_model([128, 64, 32], name=\"Teacher\")\n",
        "base_teacher.compile(optimizer='adam',\n",
        "                      loss={\"conv_prob\": \"binary_crossentropy\",\n",
        "                            \"conv_value\": \"poisson\",\n",
        "                            \"prob_logits\": None})\n",
        "base_teacher.fit(X_train, y_train, epochs=5, batch_size=64, verbose=0)\n",
        "teacher_weights = base_teacher.get_weights()\n",
        "print(\"Teacher Ready.\\n\")\n",
        "\n",
        "\n",
        "## run experiments with all sweep combinations\n",
        "results = []\n",
        "print(f\"{'RUN CONFIGURATION':<50} | {'LOSS':<8}\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "for (dropout, alpha, temp), label in generate_configs():\n",
        "    loss, _ = run_experiment(dropout, alpha, temp, X_train, y_train, teacher_weights)\n",
        "    print(f\"{label:<50} | {loss:.4f}\")\n",
        "    results.append((label, loss, dropout, alpha, temp))\n",
        "\n",
        "print(\"-\" * 70)\n",
        "best_run = min(results, key=lambda x: x[1])\n",
        "print(f\"Best Configuration: {best_run[0]}\")\n",
        "print(f\"Lowest Loss: {best_run[1]:.4f}\")\n",
        "\n",
        "best_label, best_loss, best_dropout, best_alpha, best_temp = best_run\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d58e3c2",
      "metadata": {
        "id": "5d58e3c2"
      },
      "source": [
        "### Distillation Training Loop Summary\n",
        "\n",
        "In each training batch, we compute:\n",
        "\n",
        "- Teacher predictions: Conversion probability and value from the frozen teacher (soft targets).\n",
        "- Student predictions: Corresponding outputs from the student model.\n",
        "\n",
        "---\n",
        "\n",
        "### Loss Components\n",
        "\n",
        "- Hard loss:  \n",
        "  - Binary cross-entropy between true conversion labels and student probabilities  \n",
        "  - Poisson loss between true counts and student predicted values  \n",
        "\n",
        "- Soft loss:  \n",
        "  - Mean squared error (MSE) between teacher and student outputs for both heads  \n",
        "\n",
        "- Feature matching loss (optional):  \n",
        "  - MSE between intermediate teacher (“hint”) and student (“guided”) representations, encouraging the student to mimic the teacher's internal features  \n",
        "  - Scaled by a factor gamma (set to 0 to disable)\n",
        "\n",
        "---\n",
        "\n",
        "### Optimization\n",
        "\n",
        "The total student loss is a weighted sum of hard loss, soft loss, and feature matching loss. In this example:\n",
        "- alpha = [0.1, 0.5] (part of config) balances hard vs. soft losses  \n",
        "- gamma = 0.1 lightly weights feature matching  \n",
        "\n",
        "The student is trained for a few epochs, during which the distillation loss decreases, showing that it is learning from both ground truth and teacher outputs.\n",
        "\n",
        "At this stage, the student model closely approximates the teacher. Next, we apply additional compression techniques such as quantization and pruning to further reduce model size and inference cost.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "e68d76f0",
      "metadata": {
        "id": "e68d76f0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "92d156ac",
      "metadata": {
        "id": "92d156ac"
      },
      "source": [
        "## 3. Post-Training Quantization (PTQ)\n",
        "\n",
        "Post-training quantization (PTQ) reduces model size and improves inference efficiency after training, without updating model weights. In this implementation, we use dynamic range quantization, which converts weights from float32 to int8 while keeping activations as float32. This approach typically achieves ~2-4x model size reduction with minimal accuracy loss.\n",
        "\n",
        "---\n",
        "\n",
        "### PTQ with TensorFlow Lite\n",
        "\n",
        "PTQ is applied using the TensorFlow Lite converter, which can directly convert a trained `tf.keras` model. Dynamic range quantization is the simplest form of PTQ—it only requires setting the optimization flag and does not require a representative dataset for calibration, making it straightforward to apply while still providing significant model compression.\n",
        "\n",
        "---\n",
        "\n",
        "### Setup\n",
        "\n",
        "Here, we apply PTQ to the best-performing student model from the distillation experiments:\n",
        "- Convert the Keras model to a TFLite-compatible wrapper (single output instead of dictionary)\n",
        "- Create both a float32 baseline and a quantized version using `tf.lite.Optimize.DEFAULT`\n",
        "- Compare file sizes and verify prediction accuracy\n",
        "\n",
        "This produces a compact, efficient model suitable for low-latency, production inference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "d9fe61f5",
      "metadata": {
        "id": "d9fe61f5"
      },
      "outputs": [],
      "source": [
        "def quantize_verify(student_model, X_train, y_train, config_label):\n",
        "  print(f\"Post-training quantization: {config_label}\")\n",
        "  wrapper_input = tf.keras.Input(shape=(10,), name=\"input\")\n",
        "  student_outputs = student_model(wrapper_input)\n",
        "  wrapper_model = tf.keras.Model(inputs=wrapper_input, outputs=student_outputs[\"conv_prob\"], name=\"wrapper\")\n",
        "\n",
        "  # convert to FP32 TFLite model (baseline)\n",
        "  converter_fp32 = tf.lite.TFLiteConverter.from_keras_model(wrapper_model)\n",
        "  tflite_model_fp32 = converter_fp32.convert()\n",
        "\n",
        "  # convert with dynamic range quantization (weights only)\n",
        "  converter_quant = tf.lite.TFLiteConverter.from_keras_model(wrapper_model)\n",
        "  converter_quant.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "  tflite_model_quant = converter_quant.convert()\n",
        "\n",
        "  with open(\"student_model_fp32.tflite\", \"wb\") as f:\n",
        "      f.write(tflite_model_fp32)\n",
        "  with open(\"student_model_quantized.tflite\", \"wb\") as f:\n",
        "      f.write(tflite_model_quant)\n",
        "\n",
        "  fp32_size = os.path.getsize(\"student_model_fp32.tflite\") / 1024\n",
        "  quant_size = os.path.getsize(\"student_model_quantized.tflite\") / 1024\n",
        "  compression_ratio = fp32_size / quant_size if quant_size > 0 else 0\n",
        "  size_reduction = ((fp32_size - quant_size) / fp32_size * 100) if fp32_size > 0 else 0\n",
        "\n",
        "  print(f\"   FP32 TFLite model size:       {fp32_size:.2f} KB\")\n",
        "  print(f\"   Quantized TFLite model size:  {quant_size:.2f} KB\")\n",
        "  print(f\"   Compression ratio:            {compression_ratio:.2f}x\")\n",
        "  print(f\"   Size reduction:               {size_reduction:.1f}%\")\n",
        "\n",
        "  # load interpreters\n",
        "  interpreter_fp32 = tf.lite.Interpreter(model_content=tflite_model_fp32)\n",
        "  interpreter_fp32.allocate_tensors()\n",
        "  input_details_fp32 = interpreter_fp32.get_input_details()\n",
        "  output_details_fp32 = interpreter_fp32.get_output_details()\n",
        "\n",
        "  interpreter_quant = tf.lite.Interpreter(model_content=tflite_model_quant)\n",
        "  interpreter_quant.allocate_tensors()\n",
        "  input_details_quant = interpreter_quant.get_input_details()\n",
        "  output_details_quant = interpreter_quant.get_output_details()\n",
        "\n",
        "  print(f\"\\n   {'Example':<10} | {'Original':<12} | {'FP32 TFLite':<12} | {'Quantized':<12} | {'Quant Error':<12}\")\n",
        "  print(\"   \" + \"-\"*70)\n",
        "\n",
        "  total_error = 0.0\n",
        "  num_examples = 10\n",
        "\n",
        "  for i in range(num_examples):\n",
        "      x = X_train[i:i+1]\n",
        "\n",
        "      # original model prediction\n",
        "      orig_pred = student_model(x, training=False)\n",
        "      orig_prob = float(orig_pred[\"conv_prob\"][0, 0])\n",
        "\n",
        "      # FP32 TFLite model prediction\n",
        "      interpreter_fp32.set_tensor(input_details_fp32[0]['index'], x)\n",
        "      interpreter_fp32.invoke()\n",
        "      fp32_prob = float(interpreter_fp32.get_tensor(output_details_fp32[0]['index'])[0, 0])\n",
        "\n",
        "      # quantized TFLite model prediction\n",
        "      interpreter_quant.set_tensor(input_details_quant[0]['index'], x)\n",
        "      interpreter_quant.invoke()\n",
        "      quant_prob = float(interpreter_quant.get_tensor(output_details_quant[0]['index'])[0, 0])\n",
        "\n",
        "      error = abs(orig_prob - quant_prob)\n",
        "      total_error += error\n",
        "\n",
        "      print(f\"   {i:<10} | {orig_prob:<12.4f} | {fp32_prob:<12.4f} | {quant_prob:<12.4f} | {error:<12.4f}\")\n",
        "\n",
        "  avg_error = total_error / num_examples\n",
        "  print(f\"\\n   Average quantization error: {avg_error:.4f}\")\n",
        "\n",
        "  print(\"\\n\" + \"=\"*80)\n",
        "\n",
        "  return {\n",
        "      \"fp32_size_kb\": fp32_size,\n",
        "      \"quantized_size_kb\": quant_size,\n",
        "      \"compression_ratio\": compression_ratio,\n",
        "      \"size_reduction_pct\": size_reduction,\n",
        "      \"avg_quantization_error\": avg_error\n",
        "  }"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d738a47",
      "metadata": {
        "id": "9d738a47"
      },
      "source": [
        "### PTQ Results and Validation\n",
        "\n",
        "After conversion, we compare the file sizes of the float32 and quantized TFLite models. The quantized model achieves approximately 1.75x compression (43% size reduction), reducing the model from ~13 KB to ~7.5 KB. While not the full 4x compression possible with full integer quantization, dynamic range quantization provides a good balance between compression and ease of implementation.\n",
        "\n",
        "Post-training quantization does not require retraining, making it simple to apply. The main benefit of dynamic range quantization is that it introduces minimal accuracy loss. In our validation, the average quantization error is only 0.0003, meaning predictions remain nearly identical to the original model.\n",
        "\n",
        "Finally, we validate the quantized model by running inference with a TensorFlow Lite interpreter on 10 test examples, comparing predictions from the original Keras model, the float32 TFLite model (to verify conversion accuracy), and the quantized model (to measure quantization impact). The results confirm the quantized model produces accurate outputs with negligible degradation.\n",
        "\n",
        "*Note on Loss Values*: The training loss reported may be negative due to the Poisson loss component, which can produce negative values when predictions closely match targets.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a0ea779",
      "metadata": {
        "id": "3a0ea779"
      },
      "outputs": [],
      "source": [
        "# run post-training quantization on the best model\n",
        "print(\"Retraining best configuration for quantization...\")\n",
        "print(f\"Config: Dropout={best_dropout}, Alpha={best_alpha}, Temp={best_temp}\")\n",
        "\n",
        "# retrain the best model\n",
        "_, best_student_model = run_experiment(best_dropout, best_alpha, best_temp,\n",
        "                                       X_train, y_train, teacher_weights)\n",
        "\n",
        "# run quantization and verification\n",
        "quant_results = quantize_verify(best_student_model, X_train, y_train, best_label)\n",
        "\n",
        "print(f\"\\nFinal Summary:\")\n",
        "print(f\"  Best Training Loss:        {best_loss:.4f}\")\n",
        "print(f\"  FP32 Model Size:           {quant_results['fp32_size_kb']:.2f} KB\")\n",
        "print(f\"  Quantized Model Size:      {quant_results['quantized_size_kb']:.2f} KB\")\n",
        "print(f\"  Size Reduction:            {quant_results['size_reduction_pct']:.1f}%\")\n",
        "print(f\"  Avg Quantization Error:    {quant_results['avg_quantization_error']:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "bc124541",
      "metadata": {
        "id": "bc124541"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "udphimQ40SE-"
      },
      "id": "udphimQ40SE-"
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "-iKXqddb0r2j"
      },
      "id": "-iKXqddb0r2j"
    },
    {
      "cell_type": "markdown",
      "id": "e349b170",
      "metadata": {
        "id": "e349b170"
      },
      "source": [
        "## 4. Model Pruning for Weight Sparsity\n",
        "\n",
        "Pruning reduces model size by removing low-impact weights, making the network sparse. Many neural network weights contribute little to predictions, so zeroing them out and retraining can significantly reduce model size with minimal accuracy loss.\n",
        "\n",
        "---\n",
        "\n",
        "### Pruning Workflow\n",
        "\n",
        "The code below implements magnitude-based pruning to compress the best performing student model at target sparsity levels of 30%, 50%, and 70%. It calculates layer-specific thresholds to zero out the least significant weights based on their absolute values. Then it does fine-tuning to recover accuracy. During this fine-tuning loop, it forces the zeroed out weights to remain frozen at zero. Finally it takes a sample of some predictions and compares the results to show that the prediction errors were minimal even after the highest level of pruning.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def prune_and_evaluate(student_model, X_train, y_train, sparsity_target=0.5):\n",
        "    print(f\"Magnitude-based pruning (target sparsity: {sparsity_target*100:.0f}%)\")\n",
        "\n",
        "    # clone model\n",
        "    pruned_model = tf.keras.models.clone_model(student_model)\n",
        "    pruned_model.set_weights(student_model.get_weights())\n",
        "\n",
        "    # apply magnitude-based pruning and create masks\n",
        "    pruning_masks = {}\n",
        "    total_weights_pruned = 0\n",
        "    total_weights = 0\n",
        "\n",
        "    for i, layer in enumerate(pruned_model.layers):\n",
        "        if hasattr(layer, 'kernel'):  # only prune layers with weights\n",
        "            weights = layer.get_weights()\n",
        "            kernel = weights[0]\n",
        "\n",
        "            # find threshold value below which weights will be zeroed\n",
        "            flat_weights = np.abs(kernel.flatten())\n",
        "            threshold = np.percentile(flat_weights, sparsity_target * 100)\n",
        "\n",
        "            # create mask: 0 for pruned weights, 1 for kept weights\n",
        "            mask = (np.abs(kernel) >= threshold).astype(np.float32)\n",
        "            pruned_kernel = kernel * mask\n",
        "\n",
        "            # track pruning statistics\n",
        "            weights_in_layer = kernel.size\n",
        "            pruned_in_layer = np.sum(mask == 0)\n",
        "            total_weights += weights_in_layer\n",
        "            total_weights_pruned += pruned_in_layer\n",
        "\n",
        "            # store mask and update layer weights\n",
        "            pruning_masks[layer.name] = mask\n",
        "            weights[0] = pruned_kernel\n",
        "            layer.set_weights(weights)\n",
        "\n",
        "    # fine-tune and preserve pruning\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(64)\n",
        "    for epoch in range(2):\n",
        "        for x_batch, y_batch in dataset:\n",
        "            with tf.GradientTape() as tape:\n",
        "                predictions = pruned_model(x_batch, training=True)\n",
        "\n",
        "                # calculate losses\n",
        "                prob_squeezed = tf.squeeze(predictions[\"conv_prob\"], axis=-1)\n",
        "                val_squeezed = tf.squeeze(predictions[\"conv_value\"], axis=-1)\n",
        "\n",
        "                loss = tf.reduce_mean(tf.keras.losses.binary_crossentropy(y_batch[\"conv_prob\"], prob_squeezed)) + \\\n",
        "                       tf.reduce_mean(tf.keras.losses.Poisson()(y_batch[\"conv_value\"], val_squeezed))\n",
        "\n",
        "            # apply gradients and then re-apply masks to ensure zeros stay zero\n",
        "            gradients = tape.gradient(loss, pruned_model.trainable_variables)\n",
        "            optimizer.apply_gradients(zip(gradients, pruned_model.trainable_variables))\n",
        "\n",
        "            # re-zero pruned weights after each gradient update\n",
        "            for layer in pruned_model.layers:\n",
        "                if layer.name in pruning_masks and hasattr(layer, 'kernel'):\n",
        "                    weights = layer.get_weights()\n",
        "                    weights[0] = weights[0] * pruning_masks[layer.name]\n",
        "                    layer.set_weights(weights)\n",
        "\n",
        "    def get_sparsity(model, threshold=1e-7):\n",
        "        zero_count = 0\n",
        "        total_count = 0\n",
        "        layer_stats = []\n",
        "\n",
        "        for layer in model.layers:\n",
        "            if hasattr(layer, 'kernel'):\n",
        "                weights = layer.kernel.numpy()\n",
        "                layer_zeros = np.sum(np.abs(weights) < threshold)\n",
        "                layer_total = weights.size\n",
        "                zero_count += layer_zeros\n",
        "                total_count += layer_total\n",
        "                layer_stats.append({\n",
        "                    'name': layer.name,\n",
        "                    'zeros': layer_zeros,\n",
        "                    'total': layer_total,\n",
        "                    'sparsity': (layer_zeros / layer_total * 100) if layer_total > 0 else 0\n",
        "                })\n",
        "\n",
        "        overall_sparsity = (zero_count / total_count * 100) if total_count > 0 else 0\n",
        "        return overall_sparsity, layer_stats\n",
        "\n",
        "    original_sparsity, _ = get_sparsity(student_model)\n",
        "    pruned_sparsity, layer_stats = get_sparsity(pruned_model)\n",
        "\n",
        "    print(f\"Original model sparsity:  {original_sparsity:.2f}%\")\n",
        "    print(f\"Pruned model sparsity:    {pruned_sparsity:.2f}%\")\n",
        "    print(f\"{'Layer':<20} | {'Zeros':<10} | {'Total':<10} | {'Sparsity':<10}\")\n",
        "    print(\"-\"*60)\n",
        "    for stat in layer_stats:\n",
        "        print(f\"{stat['name']:<20} | {stat['zeros']:<10} | {stat['total']:<10} | {stat['sparsity']:<10.2f}%\")\n",
        "\n",
        "    # compare predictions\n",
        "    print(\"\\nPredictions comparison on test samples...\")\n",
        "    print(f\"{'Example':<10} | {'Original':<12} | {'Pruned':<12} | {'Difference':<12}\")\n",
        "    print(\"-\"*55)\n",
        "\n",
        "    total_diff = 0.0\n",
        "    num_examples = 10\n",
        "\n",
        "    for i in range(num_examples):\n",
        "        x = X_train[i:i+1]\n",
        "\n",
        "        orig_pred = student_model(x, training=False)\n",
        "        orig_prob = float(orig_pred[\"conv_prob\"][0, 0])\n",
        "\n",
        "        pruned_pred = pruned_model(x, training=False)\n",
        "        pruned_prob = float(pruned_pred[\"conv_prob\"][0, 0])\n",
        "\n",
        "        diff = abs(orig_prob - pruned_prob)\n",
        "        total_diff += diff\n",
        "\n",
        "        print(f\"{i:<10} | {orig_prob:<12.4f} | {pruned_prob:<12.4f} | {diff:<12.4f}\")\n",
        "\n",
        "    avg_diff = total_diff / num_examples\n",
        "    print(f\"Average prediction difference: {avg_diff:.4f}\")\n",
        "    print(\"=\" * 80 + \"\\n\")\n",
        "\n",
        "    return {\n",
        "        \"pruned_model\": pruned_model,\n",
        "        \"original_sparsity\": original_sparsity,\n",
        "        \"pruned_sparsity\": pruned_sparsity,\n",
        "        \"avg_prediction_diff\": avg_diff\n",
        "    }"
      ],
      "metadata": {
        "id": "UCvMukku2M22"
      },
      "id": "UCvMukku2M22",
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sparsity_levels = [0.3, 0.5, 0.7]\n",
        "pruning_results = []\n",
        "\n",
        "for sparsity in sparsity_levels:\n",
        "    result = prune_and_evaluate(best_student_model, X_train, y_train, sparsity_target=sparsity)\n",
        "    pruning_results.append({\n",
        "        \"target_sparsity\": sparsity,\n",
        "        \"achieved_sparsity\": result[\"pruned_sparsity\"],\n",
        "        \"avg_error\": result[\"avg_prediction_diff\"]\n",
        "    })\n",
        "\n",
        "print(\"Summary:\")\n",
        "print(f\"\\n{'Target Sparsity':<20} | {'Achieved Sparsity':<20} | {'Avg Prediction Error':<20}\")\n",
        "print(\"-\"*70)\n",
        "\n",
        "for res in pruning_results:\n",
        "    print(f\"{res['target_sparsity']*100:<19.0f}% | {res['achieved_sparsity']:<19.2f}% | {res['avg_error']:<20.4f}\")"
      ],
      "metadata": {
        "id": "s7PMh1hI22WJ"
      },
      "id": "s7PMh1hI22WJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "58e9e6fd",
      "metadata": {
        "id": "58e9e6fd"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "2457f9f0",
      "metadata": {
        "id": "2457f9f0"
      },
      "source": [
        "## 5. Conclusion\n",
        "\n",
        "We presented a workflow pipeline inspired by the Google Ads conversion prediction setup. Starting from a large teacher model with multi-task outputs (conversion probability and value, optimized with sigmoid and Poisson losses), we trained a compact student model using knowledge distillation.\n",
        "\n",
        "The student was further optimized using:\n",
        "- Post-training quantization (PTQ) for 43% model size reduction with minimal accuracy loss  \n",
        "- Pruning up to ~70% sparsity with fine-tuning  \n",
        "\n",
        "Together, these steps significantly reduce model size, latency, and compute cost while maintaining strong performance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "d3a082e8",
      "metadata": {
        "id": "d3a082e8"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.13.2"
    },
    "vscode": {
      "interpreter": {
        "hash": "613da7c49a73a2227936c3a44b212d9d592039352d3665b401ad64e08b70bbf9"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}