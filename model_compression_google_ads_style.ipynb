{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "gJltvRy052pC",
      "metadata": {
        "id": "gJltvRy052pC"
      },
      "source": [
        "## ðŸ“Œ Note on Reproducibility and Scope\n",
        "\n",
        "This notebook serves as an **open-source, self-contained illustration** of the **model-compression techniques** explored in our **HMPL final project**, including **knowledge distillation, quantization, and pruning**.\n",
        "\n",
        "Due to the use of **proprietary Google infrastructure** in the original experiments, the full production pipeline cannot be released publicly. Instead, we provide a **faithful synthetic reproduction** of the training and compression workflow that mirrors the **core modeling ideas, loss formulations, and optimization strategies** used in the original project.\n",
        "\n",
        "This approach ensures **transparency, clarity, and reproducibility**, while preserving the conceptual and technical integrity of the original system.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "P8oCFIMm53Yp",
      "metadata": {
        "id": "P8oCFIMm53Yp"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "e18893f9",
      "metadata": {
        "id": "e18893f9"
      },
      "source": [
        "# Efficient Model Compression Pipeline  \n",
        "## Google Ads Conversion Prediction Example\n",
        "\n",
        "### 1. Teacher and Student Model Definition\n",
        "\n",
        "In Google Adsâ€™ internal **conversion value prediction system**, a large **teacher model** predicts conversion outcomes using a **multi-head architecture**:\n",
        "\n",
        "- **Conversion probability head (classification)**  \n",
        "- **Conversion value / count head (regression)**  \n",
        "\n",
        "The teacher model is trained with a **combination of losses**:\n",
        "\n",
        "- **Binary Cross-Entropy (sigmoid) loss** for conversion probability  \n",
        "- **Poisson negative log-likelihood loss** for conversion count or value  \n",
        "\n",
        "The Poisson log loss (also known as *negative Poisson log-likelihood*) is well suited for modeling **count data** such as conversions and serves as the **primary objective** in Googleâ€™s system.\n",
        "\n",
        "---\n",
        "\n",
        "### Student Model\n",
        "\n",
        "The **student model** is a smaller, more efficient neural network designed to **mimic the teacherâ€™s behavior**. It uses the same two-output structure:\n",
        "\n",
        "- Conversion probability  \n",
        "- Conversion value  \n",
        "\n",
        "but with a simpler architecture. Through **knowledge distillation**, the student learns to approximate the teacherâ€™s predictions while using significantly fewer parameters, enabling **faster inference and lower serving costs** in production.\n",
        "\n",
        "---\n",
        "\n",
        "### Synthetic Experimental Setup\n",
        "\n",
        "To illustrate this pipeline, we define a **synthetic conversion prediction task**:\n",
        "\n",
        "- Each training sample consists of:\n",
        "  - A random feature vector\n",
        "  - A binary label indicating whether a conversion occurred\n",
        "  - A non-negative conversion count (with many zeros, reflecting real-world sparsity)\n",
        "\n",
        "---\n",
        "\n",
        "### Model Architectures\n",
        "\n",
        "**Teacher Model**\n",
        "- Architecture: [64, 32] - Two dense hidden layers\n",
        "- Two output heads:\n",
        "  - `conv_prob`: Sigmoid activation (conversion probability)\n",
        "  - `conv_value`: Softplus activation (non-negative count for Poisson loss)\n",
        "  - `prob_logits`: Raw logits for distillation\n",
        "\n",
        "**Student Model**\n",
        "- Architecture: [32] - single hidden layer (fixed to ensure dimension compatibility with the teacher's bottleneck layer)\n",
        "- Same two output heads and activations as the teacher\n",
        "- Optimized for efficiency while maintaining predictive performance\n",
        "\n",
        "---\n",
        "### Hyperparameter Sweep Configuration\n",
        "\n",
        "We include an example config with sweeps to show how the experiments can pull from the config and sweep over the specified hyperparamters. This example is very simple compared to the more sophisticated pyplan config options at Google, but demonstrates the high-level workflow.\n",
        "\n",
        "\n",
        "This setup demonstrates how **model compression via teacherâ€“student training** can retain accuracy while significantly improving deployment efficiency in large-scale systems such as Google Ads.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "fcce0fe6",
      "metadata": {
        "id": "fcce0fe6"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import itertools\n",
        "\n",
        "# example of user config input (in reality would be separate file)\n",
        "HYPERPARAM_CONFIG = {\n",
        "    \"student\": {\n",
        "        \"dropout_rate\": [0.0, 0.1, 0.2]\n",
        "    },\n",
        "    \"distillation\": {\n",
        "        \"alpha\": [0.1, 0.5, 0.9],\n",
        "        \"temperature\": [3.0, 5.0]\n",
        "    }\n",
        "}\n",
        "\n",
        "# Fix random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# Synthetic data dimensions\n",
        "num_features = 10\n",
        "num_samples = 10000\n",
        "\n",
        "# Generate synthetic feature matrix\n",
        "X_train = np.random.normal(size=(num_samples, num_features)).astype(np.float32)\n",
        "\n",
        "# Generate synthetic conversion labels:\n",
        "# We simulate a scenario where conversion count follows a Poisson distribution based on features.\n",
        "# Define an arbitrary \"true\" weight vector for Poisson rate:\n",
        "true_w = np.random.normal(scale=0.5, size=(num_features, 1))\n",
        "# Compute a latent log-rate and apply exp to get Poisson lambda\n",
        "log_lambda = X_train.dot(true_w)  # shape (num_samples, 1)\n",
        "lambda_vals = np.exp(log_lambda).flatten()\n",
        "# Sample conversion counts from Poisson(lambda). (Clip to some max for safety)\n",
        "y_count = np.random.poisson(lam=lambda_vals).astype(np.float32)\n",
        "y_count = np.clip(y_count, 0, 20)  # limit extreme values for stability\n",
        "\n",
        "# Conversion occurred or not (binary label) â€“ 1 if count > 0 else 0\n",
        "y_conv = (y_count > 0).astype(np.float32)\n",
        "\n",
        "# Prepare targets for Keras\n",
        "y_train = {\n",
        "    \"conv_prob\": y_conv,\n",
        "    \"conv_value\": y_count,\n",
        "    \"prob_logits\": np.zeros_like(y_conv)\n",
        "}\n",
        "\n",
        "def build_model(hidden_units, dropout=0.0, name=\"Model\"):\n",
        "    inputs = tf.keras.Input(shape=(10,), name=\"features\")\n",
        "    x = inputs\n",
        "\n",
        "    if isinstance(hidden_units, int):\n",
        "        hidden_units = [hidden_units]\n",
        "\n",
        "    for i, units in enumerate(hidden_units):\n",
        "        layer_name = \"bottleneck\" if i == len(hidden_units) - 1 else f\"hidden_{i}\"\n",
        "        x = tf.keras.layers.Dense(units, activation='relu', name=layer_name)(x)\n",
        "        if dropout > 0:\n",
        "            x = tf.keras.layers.Dropout(dropout)(x)\n",
        "\n",
        "    # Outputs\n",
        "    prob_logits = tf.keras.layers.Dense(1, activation=None, name=\"prob_logits\")(x)\n",
        "    prob_output = tf.keras.layers.Activation('sigmoid', name=\"conv_prob\")(prob_logits)\n",
        "    value_output = tf.keras.layers.Dense(1, activation=tf.nn.softplus, name=\"conv_value\")(x)\n",
        "\n",
        "    # FIX: Return a Dictionary of outputs to match the keys in loss/y_train\n",
        "    return tf.keras.Model(\n",
        "        inputs=inputs,\n",
        "        outputs={\n",
        "            \"conv_prob\": prob_output,\n",
        "            \"conv_value\": value_output,\n",
        "            \"prob_logits\": prob_logits\n",
        "        },\n",
        "        name=name\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "49b46675",
      "metadata": {
        "id": "49b46675"
      },
      "source": [
        "### Explanation of the Synthetic Training Setup and Losses\n",
        "\n",
        "In this code, we generate **random input features** and construct labels to mimic a realistic conversion prediction task:\n",
        "\n",
        "- The **conversion count** `y_count` is sampled from a Poisson distribution:  \n",
        "  $$\n",
        "  y_{\\text{count}} \\sim \\text{Poisson}(\\exp(w \\cdot x))\n",
        "  $$\n",
        "- The **binary conversion label** `y_conv` is defined as:\n",
        "  $$\n",
        "  y_{\\text{conv}} = \\mathbb{1}(y_{\\text{count}} > 0)\n",
        "  $$\n",
        "\n",
        "---\n",
        "\n",
        "### Teacher Model Training Objective\n",
        "\n",
        "The teacher model is trained using **two loss functions**, corresponding to its two output heads:\n",
        "\n",
        "- **Binary Cross-Entropy Loss** for the `conv_prob` output, modeling the probability of a conversion\n",
        "- **Poisson Loss** (`tf.keras.losses.Poisson`) for the `conv_value` output, modeling the conversion count or value\n",
        "\n",
        "The Poisson loss in TensorFlow implements the **negative Poisson log-likelihood**:\n",
        "$$\n",
        "L = y_{\\text{pred}} - y_{\\text{true}} \\log(y_{\\text{pred}})\n",
        "$$\n",
        "which matches the standard Poisson log loss **up to constant terms** that do not affect optimization.\n",
        "\n",
        "---\n",
        "\n",
        "### Training Context\n",
        "\n",
        "For demonstration purposes, the teacher model is trained **briefly on synthetic data**. In a real production setting, the teacher would be **pre-trained on a massive dataset** with richer features and longer training schedules.\n",
        "\n",
        "With the teacher model trained, we now proceed to **compress the model using knowledge distillation**, transferring its predictive behavior to a smaller and more efficient student network.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a24af986",
      "metadata": {
        "id": "a24af986"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "5fdde33f",
      "metadata": {
        "id": "5fdde33f"
      },
      "source": [
        "## 2. Knowledge Distillation Training (Hard + Soft Targets)\n",
        "\n",
        "**Knowledge distillation (KD)** compresses a large **teacher** model into a smaller **student** by training the student on both:\n",
        "- **Hard targets** (ground-truth labels), and  \n",
        "- **Soft targets** (the teacherâ€™s predictions).\n",
        "\n",
        "The key idea is that the teacherâ€™s outputs encode *dark knowledge* about the underlying function, helping the student generalize better than training on labels alone.\n",
        "\n",
        "---\n",
        "\n",
        "### Distillation Loss\n",
        "\n",
        "The student is trained with a **weighted combination** of losses:\n",
        "$$\n",
        "L_{\\text{total}} = \\alpha \\, L_{\\text{hard}}(y_{\\text{true}}, y_{\\text{student}})\n",
        "+ \\beta \\, L_{\\text{soft}}(y_{\\text{teacher}}, y_{\\text{student}})\n",
        "$$\n",
        "where \\(\\alpha\\) and \\(\\beta\\) balance supervision from true labels and the teacher (often \\(\\alpha + \\beta = 1\\)).\n",
        "\n",
        "- **Hard losses**:\n",
        "  - Binary cross-entropy for conversion probability  \n",
        "  - Poisson loss for conversion count  \n",
        "- **Soft losses**:\n",
        "  - Mean squared error (MSE) between teacher and student outputs for each head  \n",
        "\n",
        "---\n",
        "\n",
        "### Optional Feature Matching\n",
        "\n",
        "We also include an optional **feature-based distillation** term (FitNets-style), where an intermediate student representation is trained to match a corresponding **teacher â€œhintâ€ layer**. This additional supervision can further improve student performance.\n",
        "\n",
        "---\n",
        "\n",
        "### Training Procedure\n",
        "\n",
        "The teacherâ€™s weights are **frozen**, and the student is trained using the combined distillation loss. Below, we illustrate this with a **custom training loop**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "c6d8cf84",
      "metadata": {
        "id": "c6d8cf84",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce8c865d-f964-4760-8834-256702443e70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Base Teacher ([64, 32])...\n",
            "Teacher Ready.\n",
            "\n",
            "RUN CONFIGURATION              | LOSS    \n",
            "--------------------------------------------------\n",
            "Dropout=0.0, Alpha=0.1, Temp=3.0                   | 1.9028\n",
            "Dropout=0.0, Alpha=0.1, Temp=5.0                   | 10.2212\n",
            "Dropout=0.0, Alpha=0.5, Temp=3.0                   | 15.9344\n",
            "Dropout=0.0, Alpha=0.5, Temp=5.0                   | 39.7678\n",
            "Dropout=0.0, Alpha=0.9, Temp=3.0                   | 33.9107\n",
            "Dropout=0.0, Alpha=0.9, Temp=5.0                   | 65.6466\n",
            "Dropout=0.1, Alpha=0.1, Temp=3.0                   | 3.1110\n",
            "Dropout=0.1, Alpha=0.1, Temp=5.0                   | 10.1808\n",
            "Dropout=0.1, Alpha=0.5, Temp=3.0                   | 15.9606\n",
            "Dropout=0.1, Alpha=0.5, Temp=5.0                   | 62.8417\n",
            "Dropout=0.1, Alpha=0.9, Temp=3.0                   | 31.7078\n",
            "Dropout=0.1, Alpha=0.9, Temp=5.0                   | 78.9286\n",
            "Dropout=0.2, Alpha=0.1, Temp=3.0                   | 3.5239\n",
            "Dropout=0.2, Alpha=0.1, Temp=5.0                   | 9.6949\n",
            "Dropout=0.2, Alpha=0.5, Temp=3.0                   | 17.4542\n",
            "Dropout=0.2, Alpha=0.5, Temp=5.0                   | 41.5633\n",
            "Dropout=0.2, Alpha=0.9, Temp=3.0                   | 33.4860\n",
            "Dropout=0.2, Alpha=0.9, Temp=5.0                   | 95.6618\n",
            "--------------------------------------------------\n",
            "Best Configuration: Dropout=0.0, Alpha=0.1, Temp=3.0\n",
            "Lowest Loss: 1.9028\n"
          ]
        }
      ],
      "source": [
        "def run_experiment(dropout, alpha, temp, X_train, y_train, teacher_weights):\n",
        "\n",
        "    # 1. Build Student\n",
        "    student_model = build_model([32], dropout=dropout, name=\"Student\")\n",
        "\n",
        "    # 2. Build Teacher\n",
        "    teacher_model = build_model([64, 32], name=\"Teacher\")\n",
        "    teacher_model.set_weights(teacher_weights)\n",
        "    teacher_model.trainable = False\n",
        "\n",
        "    # 3. Feature Extractors\n",
        "    teacher_feat_extractor = tf.keras.Model(inputs=teacher_model.inputs,\n",
        "                                            outputs=teacher_model.get_layer(\"bottleneck\").output)\n",
        "    student_feat_extractor = tf.keras.Model(inputs=student_model.inputs,\n",
        "                                            outputs=student_model.get_layer(\"bottleneck\").output)\n",
        "\n",
        "    # 4. Training Loop\n",
        "    batch_size = 64\n",
        "    epochs = 2\n",
        "    learning_rate = 0.001\n",
        "    gamma = 0.1\n",
        "\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(batch_size)\n",
        "\n",
        "    final_loss = 0.0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        epoch_loss = 0.0\n",
        "        steps = 0\n",
        "        for x_batch, y_batch in dataset:\n",
        "            # Teacher inference\n",
        "            t_preds = teacher_model(x_batch, training=False)\n",
        "            t_prob, t_val, t_logits = t_preds[\"conv_prob\"], t_preds[\"conv_value\"], t_preds[\"prob_logits\"]\n",
        "\n",
        "            with tf.GradientTape() as tape:\n",
        "                # Student training (dictionary output)\n",
        "                s_preds = student_model(x_batch, training=True)\n",
        "                s_prob = s_preds[\"conv_prob\"]\n",
        "                s_val = s_preds[\"conv_value\"]\n",
        "                s_logits = s_preds[\"prob_logits\"]\n",
        "\n",
        "                # FIX: Squeeze outputs to match label dimensions (64,) instead of (64, 1)\n",
        "                s_prob_squeezed = tf.squeeze(s_prob, axis=-1)\n",
        "                s_val_squeezed = tf.squeeze(s_val, axis=-1)\n",
        "\n",
        "                # Hard Loss (compare student output to Ground Truth labels)\n",
        "                loss_hard = tf.reduce_mean(tf.keras.losses.binary_crossentropy(y_batch[\"conv_prob\"], s_prob_squeezed)) + \\\n",
        "                            tf.reduce_mean(tf.keras.losses.Poisson()(y_batch[\"conv_value\"], s_val_squeezed))\n",
        "\n",
        "                # Soft Loss (compare student logits to Teacher logits)\n",
        "                t_prob_soft = tf.nn.sigmoid(t_logits / temp)\n",
        "                s_prob_soft = tf.nn.sigmoid(s_logits / temp)\n",
        "                # FIX: Instantiate the loss class, then call it\n",
        "                mse = tf.keras.losses.MeanSquaredError()\n",
        "                loss_soft = tf.reduce_mean(mse(t_prob_soft, s_prob_soft)) + \\\n",
        "                            tf.reduce_mean(mse(t_val, s_val))\n",
        "\n",
        "                # Feature Loss\n",
        "                feat_loss = 0.0\n",
        "                if gamma > 0:\n",
        "                    t_feat = teacher_feat_extractor(x_batch, training=False)\n",
        "                    s_feat = student_feat_extractor(x_batch, training=True)\n",
        "                    feat_loss = tf.reduce_mean(mse(t_feat, s_feat))\n",
        "\n",
        "                total_loss = (1 - alpha) * loss_hard + (alpha * (temp**2) * loss_soft) + (gamma * feat_loss)\n",
        "\n",
        "            grads = tape.gradient(total_loss, student_model.trainable_weights)\n",
        "            optimizer.apply_gradients(zip(grads, student_model.trainable_weights))\n",
        "            epoch_loss += total_loss\n",
        "            steps += 1\n",
        "\n",
        "        final_loss = float(epoch_loss / steps)\n",
        "\n",
        "    return final_loss\n",
        "\n",
        "\n",
        "def generate_configs():\n",
        "    dropout_list = HYPERPARAM_CONFIG['student']['dropout_rate']\n",
        "    alpha_list = HYPERPARAM_CONFIG['distillation']['alpha']\n",
        "    temp_list = HYPERPARAM_CONFIG['distillation']['temperature']\n",
        "\n",
        "    for dropout, alpha, temp in itertools.product(dropout_list, alpha_list, temp_list):\n",
        "        label = f\"Dropout={dropout}, Alpha={alpha}, Temp={temp}\"\n",
        "        yield (dropout, alpha, temp), label\n",
        "\n",
        "\n",
        "## train teacher (stays constant)\n",
        "print(\"Training Base Teacher ([64, 32])...\")\n",
        "base_teacher = build_model([64, 32], name=\"Teacher\")\n",
        "# Using Dictionary Loss to match the new Dictionary Output\n",
        "base_teacher.compile(optimizer='adam',\n",
        "                      loss={\"conv_prob\": \"binary_crossentropy\",\n",
        "                            \"conv_value\": \"poisson\",\n",
        "                            \"prob_logits\": None})\n",
        "base_teacher.fit(X_train, y_train, epochs=5, batch_size=64, verbose=0)\n",
        "teacher_weights = base_teacher.get_weights()\n",
        "print(\"Teacher Ready.\\n\")\n",
        "\n",
        "\n",
        "## run experiments with all sweep combinations\n",
        "results = []\n",
        "print(f\"{'RUN CONFIGURATION':<30} | {'LOSS':<8}\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "for (dropout, alpha, temp), label in generate_configs():\n",
        "    loss = run_experiment(dropout, alpha, temp, X_train, y_train, teacher_weights)\n",
        "    print(f\"{label:<50} | {loss:.4f}\")\n",
        "    results.append((label, loss))\n",
        "\n",
        "print(\"-\" * 50)\n",
        "best_run = min(results, key=lambda x: x[1])\n",
        "print(f\"Best Configuration: {best_run[0]}\")\n",
        "print(f\"Lowest Loss: {best_run[1]:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d58e3c2",
      "metadata": {
        "id": "5d58e3c2"
      },
      "source": [
        "### Distillation Training Loop Summary\n",
        "\n",
        "In each training batch, we compute:\n",
        "\n",
        "- **Teacher predictions**: Conversion probability and value from the frozen teacher (soft targets).\n",
        "- **Student predictions**: Corresponding outputs from the student model.\n",
        "\n",
        "---\n",
        "\n",
        "### Loss Components\n",
        "\n",
        "- **Hard loss**:  \n",
        "  - Binary cross-entropy between true conversion labels and student probabilities  \n",
        "  - Poisson loss between true counts and student predicted values  \n",
        "\n",
        "- **Soft loss**:  \n",
        "  - Mean squared error (MSE) between teacher and student outputs for both heads  \n",
        "\n",
        "- **Feature matching loss (optional)**:  \n",
        "  - MSE between intermediate teacher (â€œhintâ€) and student (â€œguidedâ€) representations, encouraging the student to mimic the teacherâ€™s internal features  \n",
        "  - Scaled by a factor \\(\\gamma\\) (set to 0 to disable)\n",
        "\n",
        "---\n",
        "\n",
        "### Optimization\n",
        "\n",
        "The **total student loss** is a weighted sum of hard loss, soft loss, and feature matching loss. In this example:\n",
        "- \\(\\alpha = 0.5\\) balances hard vs. soft losses  \n",
        "- \\(\\gamma = 0.1\\) lightly weights feature matching  \n",
        "\n",
        "The student is trained for a few epochs, during which the distillation loss decreases, showing that it is learning from both ground truth and teacher outputs.\n",
        "\n",
        "---\n",
        "\n",
        "At this stage, the student model closely approximates the teacher. Next, we apply **additional compression techniques** such as quantization and pruning to further reduce model size and inference cost.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e68d76f0",
      "metadata": {
        "id": "e68d76f0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "92d156ac",
      "metadata": {
        "id": "92d156ac"
      },
      "source": [
        "## 3. Post-Training Quantization (PTQ)\n",
        "\n",
        "**Post-training quantization (PTQ)** reduces model size and improves inference efficiency *after training*, without updating model weights. Typically, weights (and optionally activations) are converted from **float32 to int8**, often achieving up to **~4Ã— model size reduction** with minimal accuracy loss.\n",
        "\n",
        "---\n",
        "\n",
        "### PTQ with TensorFlow Lite\n",
        "\n",
        "PTQ is applied using the **TensorFlow Lite converter**, which can directly convert a trained `tf.keras` model.\n",
        "\n",
        "To quantize **activations** (not just weights), the converter requires a **representative dataset** for calibration. This dataset is used to estimate activation ranges (min/max) and compute appropriate quantization scales and zero-points.\n",
        "\n",
        "---\n",
        "\n",
        "### Setup\n",
        "\n",
        "Here, we apply PTQ to the **trained student model**:\n",
        "- Convert the Keras model to **TFLite int8 format**\n",
        "- Use a small subset of the training data as the **representative dataset** for calibration\n",
        "\n",
        "This produces a compact, efficient model suitable for low-latency, production inference.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d9fe61f5",
      "metadata": {
        "id": "d9fe61f5"
      },
      "outputs": [],
      "source": [
        "# 1. Convert the trained student model to a float32 TFLite model (for size comparison)\n",
        "converter_fp32 = tf.lite.TFLiteConverter.from_keras_model(student_model)\n",
        "tflite_model_fp32 = converter_fp32.convert()\n",
        "\n",
        "# 2. Convert the student model to an int8 quantized TFLite model (post-training quantization)\n",
        "converter_int8 = tf.lite.TFLiteConverter.from_keras_model(student_model)\n",
        "converter_int8.optimizations = [tf.lite.Optimize.DEFAULT]  # enable default optimizations (which include quantization)\n",
        "# Provide a representative dataset generator for full integer quantization\n",
        "def representative_data_gen():\n",
        "    for i in range(100):\n",
        "        # Take random batches from training data\n",
        "        sample = X_train[i:i+1]\n",
        "        yield [sample.astype(np.float32)]\n",
        "converter_int8.representative_dataset = representative_data_gen\n",
        "# Set the supported ops and ensure INT8 quantization for inputs/outputs\n",
        "converter_int8.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
        "converter_int8.inference_input_type = tf.int8\n",
        "converter_int8.inference_output_type = tf.int8\n",
        "\n",
        "tflite_model_int8 = converter_int8.convert()\n",
        "\n",
        "# Save TFLite models to disk to compare sizes\n",
        "with open(\"student_model_fp32.tflite\", \"wb\") as f:\n",
        "    f.write(tflite_model_fp32)\n",
        "with open(\"student_model_int8.tflite\", \"wb\") as f:\n",
        "    f.write(tflite_model_int8)\n",
        "\n",
        "# Compare file sizes\n",
        "import os\n",
        "fp32_size = os.path.getsize(\"student_model_fp32.tflite\") / 1024\n",
        "int8_size = os.path.getsize(\"student_model_int8.tflite\") / 1024\n",
        "print(f\"FP32 TFLite model size: {fp32_size:.2f} KB\")\n",
        "print(f\"INT8 TFLite model size: {int8_size:.2f} KB\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d738a47",
      "metadata": {
        "id": "9d738a47"
      },
      "source": [
        "### PTQ Results and Validation\n",
        "\n",
        "After conversion, we compare the **file sizes** of the float32 and int8 TFLite models. As expected, the **int8 quantized model** is approximately **4Ã— smaller**, since weights are stored in 8-bit rather than 32-bit precision.\n",
        "\n",
        "Post-training quantization does **not require retraining**, making it simple to apply. The main trade-off is a potential **small accuracy drop** due to quantization error in weights and activations. In practice, well-calibrated quantized models typically retain accuracy close to the float baseline for many tasks.\n",
        "\n",
        "Finally, we validate the quantized model by running inference with a **TensorFlow Lite interpreter** to confirm it produces reasonable outputs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a0ea779",
      "metadata": {
        "id": "3a0ea779"
      },
      "outputs": [],
      "source": [
        "# Verify the quantized model on a couple of examples\n",
        "interpreter = tf.lite.Interpreter(model_content=tflite_model_int8)\n",
        "interpreter.allocate_tensors()\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "\n",
        "# Test on a few samples\n",
        "for i in range(3):\n",
        "    x = X_train[i:i+1]\n",
        "    # Quantize the input from float32 to int8\n",
        "    input_scale, input_zero_point = input_details[0]['quantization']\n",
        "    x_int8 = (x / input_scale + input_zero_point).astype(np.int8)\n",
        "    interpreter.set_tensor(input_details[0]['index'], x_int8)\n",
        "    interpreter.invoke()\n",
        "    # Dequantize output back to float32 for interpretation\n",
        "    output_prob = interpreter.get_tensor(output_details[0]['index'])\n",
        "    output_value = interpreter.get_tensor(output_details[1]['index'])\n",
        "    print(f\"Example {i}: conv_prob={output_prob[0][0]:.3f}, conv_value={output_value[0][0]:.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "24e5fa42",
      "metadata": {
        "id": "24e5fa42"
      },
      "source": [
        "This step runs the **quantized student model** on sample inputs and prints the predicted **conversion probability** and **conversion value**. Comparing these outputs with the original float32 model shows that predictions remain **very close**, differing only by small quantization rounding errors.\n",
        "\n",
        "---\n",
        "\n",
        "### Summary\n",
        "\n",
        "Post-training quantization significantly improves efficiency by making the student model **much smaller and faster**, typically achieving a **~4Ã— reduction in model size** with int8 weights. This makes PTQ a simple and effective optimization for production deployment.\n",
        "\n",
        "If the small accuracy loss from quantization is unacceptable, the next step would be to apply **quantization-aware training (QAT)**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc124541",
      "metadata": {
        "id": "bc124541"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "e516915f",
      "metadata": {
        "id": "e516915f"
      },
      "source": [
        "## 4. Quantization-Aware Training (QAT)\n",
        "\n",
        "**Quantization-aware training (QAT)** simulates low-bit (e.g., int8) quantization *during training* by inserting **fake quantization operations** for weights and activations. This allows the model to **adapt to quantization effects** and preserve accuracy after conversion.\n",
        "\n",
        "Compared to PTQ, QAT typically achieves **better final accuracy**, at the cost of **additional fine-tuning time**.\n",
        "\n",
        "---\n",
        "\n",
        "### QAT with TensorFlow Model Optimization Toolkit\n",
        "\n",
        "We apply QAT to the **student model** using the TensorFlow Model Optimization Toolkit (TF-MOT):\n",
        "\n",
        "- The student model is wrapped with  \n",
        "  `tfmot.quantization.keras.quantize_model`\n",
        "- Fake quantization layers are inserted (visible as `quant_` layers in the model summary)\n",
        "- The quantization-aware model is **fine-tuned for a few epochs**\n",
        "- The trained model is then converted to a **TFLite int8 model**\n",
        "\n",
        "This produces a quantized model that is both **compact and more accurate** than PTQ alone, making it suitable for accuracy-sensitive production deployments.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d9b8b7d1",
      "metadata": {
        "id": "d9b8b7d1"
      },
      "outputs": [],
      "source": [
        "!pip install -q tensorflow-model-optimization\n",
        "\n",
        "import tensorflow_model_optimization as tfmot\n",
        "\n",
        "# Create a quantization-aware model from the student model\n",
        "quantize_model = tfmot.quantization.keras.quantize_model\n",
        "q_aware_student = quantize_model(student_model)  # this wraps the student model\n",
        "\n",
        "# Compile the QAT model with the same losses as the original (two outputs)\n",
        "q_aware_student.compile(optimizer=tf.keras.optimizers.Adam(0.001),\n",
        "                        loss={\"conv_prob\": \"binary_crossentropy\", \"conv_value\": tf.keras.losses.Poisson()})\n",
        "\n",
        "q_aware_student.summary()  # to show quantized layers (optional)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fcbd13a2",
      "metadata": {
        "id": "fcbd13a2"
      },
      "outputs": [],
      "source": [
        "# Fine-tune the quantization-aware model on a subset of data (to simulate QAT)\n",
        "X_subset = X_train[:2000]\n",
        "y_subset = { \"conv_prob\": y_conv[:2000].astype(np.float32), \"conv_value\": y_count[:2000].astype(np.float32) }\n",
        "\n",
        "# Train for a couple of epochs\n",
        "q_aware_student.fit(X_subset, y_subset, batch_size=64, epochs=3, verbose=0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa985982",
      "metadata": {
        "id": "fa985982"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f354dce4",
      "metadata": {
        "id": "f354dce4"
      },
      "outputs": [],
      "source": [
        "# Convert the quantization-aware trained model to a fully quantized TFLite model\n",
        "converter_qat = tf.lite.TFLiteConverter.from_keras_model(q_aware_student)\n",
        "converter_qat.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "# (No representative dataset needed, as ranges are already determined by QAT)\n",
        "tflite_model_qat = converter_qat.convert()\n",
        "\n",
        "with open(\"student_model_qat_int8.tflite\", \"wb\") as f:\n",
        "    f.write(tflite_model_qat)\n",
        "\n",
        "qat_int8_size = os.path.getsize(\"student_model_qat_int8.tflite\") / 1024\n",
        "print(f\"QAT INT8 TFLite model size: {qat_int8_size:.2f} KB\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f0812f8",
      "metadata": {
        "id": "2f0812f8"
      },
      "source": [
        "### QAT Results and Comparison\n",
        "\n",
        "The **QAT-generated int8 model** has a file size similar to the PTQ int8 model, since both are fully quantized to 8-bit. The key advantage of QAT is **accuracy**, not size.\n",
        "\n",
        "By training with simulated quantization effects, the QAT model typically achieves accuracy **much closer to the original float model** than PTQ, especially for more complex or sensitive tasks. In many production settings, the accuracy drop with QAT is **negligible or eliminated**.\n",
        "\n",
        "We can validate this by evaluating the QAT model on a test set or comparing its predictions against the float model. In this simplified example, differences may be small due to the toy setup, but in real systems QAT is often essential when PTQ accuracy loss is unacceptable.\n",
        "\n",
        "---\n",
        "\n",
        "**Recap:** QAT integrates quantization into training using TF-MOT, followed by conversion to a fully int8 TFLite model. The result is a compact model with **~4Ã— size reduction** and accuracy that is often within **~1% (or equal)** to the float baseline.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b623a10",
      "metadata": {
        "id": "6b623a10"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "e349b170",
      "metadata": {
        "id": "e349b170"
      },
      "source": [
        "## 5. Model Pruning for Weight Sparsity\n",
        "\n",
        "**Pruning** reduces model size by removing low-impact weights, making the network sparse. Many neural network weights contribute little to predictions, so zeroing them out and retraining can significantly reduce model size with minimal accuracy loss. In practice, pruning can yield **~3Ã— size reduction**, and when combined with quantization, up to **~10Ã— smaller models**.\n",
        "\n",
        "---\n",
        "\n",
        "### Pruning Workflow\n",
        "\n",
        "1. **Sensitivity Analysis**  \n",
        "   Test different sparsity levels (e.g., 10â€“90%) for individual layers or the full model and measure performance degradation. This helps identify which layers are more or less sensitive to pruning.\n",
        "\n",
        "2. **Pruning + Fine-Tuning**  \n",
        "   Gradually prune weights to a target sparsity and **fine-tune the model**, typically with a smaller learning rate, to recover lost accuracy.\n",
        "\n",
        "---\n",
        "\n",
        "### Demonstration Plan\n",
        "\n",
        "We first perform a **simple layer-wise sensitivity test** by pruning a fixed percentage of weights in each dense layer of the student model and measuring the change in validation loss. Based on this, we then apply **structured pruning** using the TensorFlow Model Optimization Toolkit (TF-MOT) to sparsify the student model while maintaining performance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8bc11410",
      "metadata": {
        "id": "8bc11410"
      },
      "outputs": [],
      "source": [
        "# Create a validation/test set for evaluation\n",
        "X_val = X_train[-1000:]\n",
        "y_val = { \"conv_prob\": y_conv[-1000:].astype(np.float32), \"conv_value\": y_count[-1000:].astype(np.float32) }\n",
        "\n",
        "# Compile the student model for evaluation (using the same losses)\n",
        "student_model.compile(loss={\"conv_prob\": \"binary_crossentropy\", \"conv_value\": tf.keras.losses.Poisson()})\n",
        "\n",
        "# Evaluate baseline performance\n",
        "baseline_loss = student_model.evaluate(X_val, y_val, verbose=0)\n",
        "print(f\"Baseline model loss (total): {baseline_loss:.4f}\")\n",
        "\n",
        "# Sensitivity analysis: prune 50% weights in each Dense layer and measure loss\n",
        "for layer in student_model.layers:\n",
        "    if isinstance(layer, tf.keras.layers.Dense):\n",
        "        # Skip output layers for pruning test (focus on hidden layers)\n",
        "        if layer.name in [\"conv_prob\", \"conv_value\"]:\n",
        "            continue\n",
        "        original_weights = layer.get_weights()  # [kernel, bias]\n",
        "        if not original_weights:  # if layer has no weights (shouldnâ€™t happen for Dense)\n",
        "            continue\n",
        "        W, b = original_weights\n",
        "        # Determine threshold for 50% sparsity (median of absolute weights)\n",
        "        threshold = np.percentile(np.abs(W), 50)\n",
        "        W_pruned = np.where(np.abs(W) < threshold, 0, W)\n",
        "        layer.set_weights([W_pruned, b])\n",
        "        loss_after = student_model.evaluate(X_val, y_val, verbose=0)\n",
        "        print(f\"After pruning 50% of weights in layer '{layer.name}': loss = {loss_after:.4f}\")\n",
        "        # Restore original weights\n",
        "        layer.set_weights(original_weights)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a314031",
      "metadata": {
        "id": "0a314031"
      },
      "source": [
        "### Layer-Wise Pruning Sensitivity Analysis\n",
        "\n",
        "This code iterates over each **hidden Dense layer** of the student model and performs a simple pruning test:\n",
        "\n",
        "- Computes the **50th percentile** of absolute weight values in the layer  \n",
        "- Sets weights below this threshold to zero, creating ~**50% sparsity**\n",
        "- Evaluates the model loss on a **validation set** with the layer pruned\n",
        "- Restores the original weights before moving to the next layer\n",
        "\n",
        "By comparing the printed losses, we can identify which layers are **most sensitive to pruning**â€”larger loss increases indicate layers that should be pruned more conservatively.\n",
        "\n",
        "In practice, this analysis would be extended to multiple sparsity levels (e.g., 10%â€“90%) to determine the **maximum acceptable pruning** per layer (such as keeping loss degradation under ~2%).\n",
        "\n",
        "Next, we apply **gradual pruning** using TensorFlow Model Optimization Toolkitâ€™s pruning API, which incrementally increases sparsity during fine-tuning and typically preserves accuracy better than one-shot pruning.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03b168af",
      "metadata": {
        "id": "03b168af"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c4c53a1",
      "metadata": {
        "id": "4c4c53a1"
      },
      "outputs": [],
      "source": [
        "import tensorflow_model_optimization as tfmot\n",
        "\n",
        "# Set up pruning parameters: gradually prune to 80% sparsity over 5 epochs\n",
        "pruning_epochs = 5\n",
        "steps_per_epoch = int(np.ceil(X_train.shape[0] / 64))\n",
        "prune_schedule = tfmot.sparsity.keras.PolynomialDecay(initial_sparsity=0.0,\n",
        "                                                      final_sparsity=0.8,\n",
        "                                                      begin_step=0,\n",
        "                                                      end_step=pruning_epochs * steps_per_epoch)\n",
        "\n",
        "pruning_wrapper = tfmot.sparsity.keras.prune_low_magnitude\n",
        "pruned_model = pruning_wrapper(student_model, pruning_schedule=prune_schedule)\n",
        "\n",
        "# Compile the pruned model (use same loss and optimizer)\n",
        "pruned_model.compile(optimizer=tf.keras.optimizers.Adam(0.001),\n",
        "                     loss={\"conv_prob\": \"binary_crossentropy\", \"conv_value\": tf.keras.losses.Poisson()})\n",
        "\n",
        "# Fine-tune the model with pruning\n",
        "callback = tfmot.sparsity.keras.UpdatePruningStep()  # updates pruning step each epoch\n",
        "pruned_model.fit(X_train, y_train, batch_size=64, epochs=pruning_epochs, callbacks=[callback], verbose=0)\n",
        "\n",
        "# Strip pruning wrappers to get a normal model with pruned weights\n",
        "pruned_model = tfmot.sparsity.keras.strip_pruning(pruned_model)\n",
        "\n",
        "# Check final sparsity\n",
        "total_weights = 0\n",
        "zeros = 0\n",
        "for weight in pruned_model.get_weights():\n",
        "    total_weights += weight.size\n",
        "    zeros += np.sum(weight == 0)\n",
        "print(f\"Final model sparsity: {zeros/total_weights:.2%} of weights are zero.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8fdbcce5",
      "metadata": {
        "id": "8fdbcce5"
      },
      "source": [
        "### Gradual Pruning and Results\n",
        "\n",
        "This code applies **gradual magnitude pruning** using a polynomial decay schedule:\n",
        "\n",
        "- Sparsity increases smoothly from **0% to 80%** over several epochs, allowing the model to adapt as weights are removed.\n",
        "- `prune_low_magnitude` wraps the model layers with pruning logic, and the `UpdatePruningStep` callback updates sparsity during training.\n",
        "- After fine-tuning, `strip_pruning` removes pruning wrappers, leaving a standard Keras model with many weights set to zero.\n",
        "\n",
        "The final sparsity check confirms that the model reaches **~80% zero weights** (minor deviations are expected since biases are typically not pruned).\n",
        "\n",
        "---\n",
        "\n",
        "### Evaluation and Extensions\n",
        "\n",
        "The pruned model should be evaluated to ensure performance remains acceptable. If accuracy drops too much, the target sparsity or pruning schedule can be adjusted, or the model can be further fine-tuned at a lower learning rate.\n",
        "\n",
        "**Notes:**\n",
        "- Pruning can be **combined with quantization** (e.g., PTQ or QAT) for much higher compression, often up to **~10Ã— size reduction**.\n",
        "- This example uses **unstructured pruning** (individual weights). Hardware speedups depend on sparse execution support; structured pruning can yield clearer speed gains but may affect accuracy more.\n",
        "- Adding **L1 regularization** during initial training can improve prune-ability, though it was not used here.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "58e9e6fd",
      "metadata": {
        "id": "58e9e6fd"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "2457f9f0",
      "metadata": {
        "id": "2457f9f0"
      },
      "source": [
        "## 6. Conclusion\n",
        "\n",
        "We presented an end-to-end **model compression pipeline** inspired by Google Ads conversion prediction. Starting from a large **teacher model** with multi-task outputs (conversion probability and value, optimized with sigmoid and Poisson losses), we trained a compact **student model** using knowledge distillation.\n",
        "\n",
        "The student was further optimized using:\n",
        "- **Post-training quantization (PTQ)** for ~4Ã— model size reduction with minimal accuracy loss  \n",
        "- **Quantization-aware training (QAT)** to better preserve accuracy under int8 quantization  \n",
        "- **Weight pruning**, including sensitivity analysis and gradual pruning to ~80% sparsity with fine-tuning  \n",
        "\n",
        "Together, these steps significantly reduce **model size, latency, and compute cost** while maintaining strong performance. This mirrors common large-scale production practices: train a strong model, distill it, and deploy a **small, efficient, and accurate** version suitable for real-world inference environments such as web services or mobile devices.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3a082e8",
      "metadata": {
        "id": "d3a082e8"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.13.2"
    },
    "vscode": {
      "interpreter": {
        "hash": "613da7c49a73a2227936c3a44b212d9d592039352d3665b401ad64e08b70bbf9"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}